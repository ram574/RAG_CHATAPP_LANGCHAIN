{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b438541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 221 pages from merged file.\n",
      "Splitted the merged document into 779 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# 1. Merge PDFs (already in your notebook)\n",
    "from PyPDF2 import PdfMerger\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "import getpass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "pdf_folder = \"/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/Input_Data/health-plan\"\n",
    "output_pdf_path = \"/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/Output_Data/merged_output.pdf\"\n",
    "\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "merger = PdfMerger()\n",
    "for pdf in pdf_files:\n",
    "    merger.append(pdf)\n",
    "merger.write(output_pdf_path)\n",
    "merger.close()\n",
    "\n",
    "# 2. Load merged PDF\n",
    "loader = PyPDFLoader(output_pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from merged file.\")\n",
    "\n",
    "# 3. Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=850,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "print(f\"Splitted the merged document into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# 4. Embed and store with Azure OpenAI\n",
    "## insert the LLM and embedding models here\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(all_splits, embedding_model)\n",
    "vectorstore.save_local(\"/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91942e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1324ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4279288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'PyPDF2',\n",
       " 'creator': 'PyPDF',\n",
       " 'creationdate': '',\n",
       " 'source': '/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/Output_Data/merged_output.pdf',\n",
       " 'total_pages': 221,\n",
       " 'page': 2,\n",
       " 'page_label': '3',\n",
       " 'start_index': 0,\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "all_splits[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13fd0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vectorstore = InMemoryVectorStore(embedding_model)\n",
    "_ = vectorstore.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c993792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3604f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m     query: Annotated[\u001b[38;5;28mstr\u001b[39m, ..., \u001b[33m\"\u001b[39m\u001b[33mSearch query to run.\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m     section: Annotated[\n\u001b[32m     12\u001b[39m         Literal[\u001b[33m\"\u001b[39m\u001b[33mbeginning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmiddle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mend\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m         ...,\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSection to query.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mState\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mTypedDict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSearch\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mState\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m question: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m     19\u001b[39m query: Search\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m context: List[\u001b[43mDocument\u001b[49m]\n\u001b[32m     21\u001b[39m answer: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing_extensions import Annotated, List, TypedDict\n",
    "from typing import Literal\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(state: State, vectorstore):\n",
    "    \"\"\"Retrieve relevant documents based on the query and section.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vectorstore.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the mission of Contoso Electronics?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (call_eW8UNS14sCcJWqyIrP5VMBDo)\n",
      " Call ID: call_eW8UNS14sCcJWqyIrP5VMBDo\n",
      "  Args:\n",
      "    query: mission of Contoso Electronics\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'source': '/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/Output_Data/merged_output.pdf', 'total_pages': 221, 'page': 217, 'page_label': '218', 'start_index': 0, 'section': 'end'}\n",
      "Content: Contoso Electronics \n",
      "Plan and Benefit Packages\n",
      "\n",
      "Source: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'source': '/home/tulasiram/ubuntu_server/RAG_ChatApp_LangChain/Output_Data/merged_output.pdf', 'total_pages': 221, 'page': 220, 'page_label': '221', 'start_index': 774, 'section': 'end'}\n",
      "Content: Health Plus and Northwind Standard. We \n",
      "are confident that you will find the right plan for you and \n",
      "your family. Thank you for choosing Contoso Electronics!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What is the mission of Contoso Electronics?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbe21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b337f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86190e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
